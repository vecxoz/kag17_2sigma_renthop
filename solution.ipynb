{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle 2017: Two Sigma Connect: Rental Listing Inquiries [Competition](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)\n",
    "\n",
    "** * Silver solution (118-th place) * **  \n",
    "** * Original submission is [here](https://github.com/vecxoz/kag17_2sigma_renthop/tree/master/original_best_submission) * **  \n",
    "** * Public LB: 0.50762 * **  \n",
    "** * Private LB: 0.50665 * **  \n",
    "** * Author: Igor Ivanov ([vecxoz](https://www.kaggle.com/vecxoz)) * **  \n",
    "** * Email: vecxoz@gmail.com * **  \n",
    "** * MIT License * **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Reproduce\n",
    "\n",
    "* Submission score may slightly vary depending on versions of packages, but should be around 120-th place\n",
    "* You need 8 GB RAM\n",
    "* On machine with 4 cores solution runs about 2 hours\n",
    "* To reproduce solution basically you need Ubuntu, Java and Python 3 with  \n",
    "  NumPy, Pandas, SciPy, Scikit-learn and XGBoost (maybe I missed something)\n",
    "* You can deploy ML environment on Ubuntu for Python 3 using this [script](https://github.com/vecxoz/vecsnip/blob/master/deploy_cloud_ml_ubuntu_python_no_gpu.sh)\n",
    "* Clone (or download) repository [https://github.com/vecxoz/kag17_2sigma_renthop](https://github.com/vecxoz/kag17_2sigma_renthop). You will have dir `kag17_2sigma_renthop`\n",
    "* Put `train.json` and `test.json` files into `kag17_2sigma_renthop/data`. You can download this files from competition [data page](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/data)\n",
    "* Run `$ python3 solution.py` or just run all cells of this notebook `solution.ipynb`\n",
    "* Files `solution.py` and  `solution.ipynb` contain completely identical Python code\n",
    "* After script is complete you will have submission file: `kag17_2sigma_renthop/reproduced_submission/reproduced_submission.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The dataset for this competition is just amazing.  \n",
    "We have all kinds of features: numerical, categorical, geospatial (lat/lon), text, pictures...  \n",
    "Just endless possibilities for feature engineering.  \n",
    "Let's look at some training example:  \n",
    "```\n",
    ">>> train_df.iloc[12]\n",
    "bathrooms                                                   1.000000\n",
    "bedrooms                                                           2\n",
    "building_id                         67c9b420da4a365bc26a6cd0ef4a5320\n",
    "created                                          2016-04-19 05:37:25\n",
    "description        ***LOW FEE. Beautiful CHERRY OAK WOODEN FLOORS...\n",
    "display_address                                            E 38th St\n",
    "features            [Doorman, Elevator, Laundry in Building, No Fee]\n",
    "interest_level                                                  high\n",
    "latitude                                                   40.748800\n",
    "listing_id                                                   6895442\n",
    "longitude                                                 -73.977000\n",
    "manager_id                          537e06890f6a86dbb70c187db5be4d55\n",
    "photos             [https://photos.renthop.com/2/6895442_34d617a5...\n",
    "price                                                           3000\n",
    "street_address                                         137 E 38th St\n",
    "```\n",
    "The final submission is an ensemble (weighted average) of 3 first-level models.  \n",
    "Each first-level model is meta-model by nature itself.  \n",
    "First-level models are built based on the concept of 'mixed stacking':\n",
    "* fit some model on dataset\n",
    "* predict dataset\n",
    "* append predictions to dataset\n",
    "* fit some other model on dataset + predictions  \n",
    "\n",
    "Algorithms used:\n",
    "* [XGBoost](https://github.com/dmlc/xgboost)\n",
    "* [StackNet](https://github.com/kaz-Anova/StackNet)\n",
    "* [Extra Trees](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](https://raw.githubusercontent.com/vecxoz/kag17_2sigma_renthop/master/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "from subprocess import check_output\n",
    "\n",
    "# Math stack\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress = True)\n",
    "import pandas as pd\n",
    "# pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "pd.options.mode.chained_assignment = None  # default = 'warn'\n",
    "from scipy import sparse\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Preprocessing and scoring\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Text (vectorizing, stamming, sentiment)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# Models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Default value to fill NaN\n",
    "fill_val = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "train_df = pd.read_json(data_dir + 'train.json')\n",
    "test_df = pd.read_json(data_dir + 'test.json')\n",
    "subm_df = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "# Load \"magic feature\"\n",
    "time_df = pd.read_csv(data_dir + 'listing_image_time.csv')\n",
    "# Rename columns\n",
    "time_df.columns = ['listing_id', 'timestamp']\n",
    "#\n",
    "print(train_df.shape) # (49352, 15)\n",
    "print(test_df.shape) # (74659, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine train and test to simplify feature calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 16)\n"
     ]
    }
   ],
   "source": [
    "y_col = 'interest_level'\n",
    "r, c = train_df.shape\n",
    "test_df.loc[:, y_col] = 'na'\n",
    "tt_df = pd.concat([train_df, test_df], ignore_index = True)\n",
    "# Merge with \"magic feature\"\n",
    "tt_df = pd.merge(tt_df, time_df, on = 'listing_id', how = 'left')\n",
    "print(tt_df.shape) # (124011, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal outlier correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bathrooms\n",
    "tt_df.loc[69023, 'bathrooms'] = 2 # was 112\n",
    "tt_df.loc[72329, 'bathrooms'] = 2 # was 20\n",
    "tt_df.loc[113071, 'bathrooms'] = 2 # was 20\n",
    "tt_df.loc[1990, 'bathrooms'] = 1 # was 10\n",
    "\n",
    "# lat/lon - just another city - e.g. LA - leave as is\n",
    "\n",
    "# price\n",
    "tt_df.loc[25538, 'price'] = 1025 # was 111111 # real number from dscription\n",
    "tt_df.loc[tt_df['price'] > 100000, 'price'] = 100000 # low interest_level for all\n",
    "\n",
    "# timestamp\n",
    "tt_df.loc[35264, 'timestamp'] = 1479787252 # was 1491289977 (only one record from april) # replace with last timestamp excluding this record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 60)\n",
      "NaN: False\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Features denoting presence of NaNs, zeros, outliers\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['building_id_is_zero'] = (tt_df['building_id'].apply(len) == 1).astype(np.int64)\n",
    "#---------------------------------------------------------------------------\n",
    "# Count/len\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['num_photos'] = tt_df['photos'].apply(len) # number of photos\n",
    "tt_df['num_features'] = tt_df['features'].apply(len) # number of 'features'\n",
    "tt_df['num_description_words'] = tt_df['description'].apply( lambda x: len(x.split(' ')) ) # number of words in description\n",
    "#---------------------------------------------------------------------------\n",
    "# Date/Time\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['created'] = pd.to_datetime(tt_df['created']) # convert the created column to datetime \n",
    "# tt_df['year'] = tt_df['created'].dt.year # year is constant for this dataset\n",
    "tt_df['month'] = tt_df['created'].dt.month\n",
    "tt_df['day'] = tt_df['created'].dt.day\n",
    "tt_df['hour'] = tt_df['created'].dt.hour\n",
    "#---------------------------------------------------------------------------\n",
    "# Rooms\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['bad_plus_bath'] = tt_df['bedrooms'] + tt_df['bathrooms']\n",
    "tt_df['more_bed'] = (tt_df['bedrooms'] > tt_df['bathrooms']).astype(np.int64)\n",
    "tt_df['more_bath'] = (tt_df['bedrooms'] < tt_df['bathrooms']).astype(np.int64)\n",
    "tt_df['bed_bath_equal'] = (tt_df['bedrooms'] == tt_df['bathrooms']).astype(np.int64)\n",
    "tt_df['bed_bath_diff'] = tt_df['bedrooms'] - tt_df['bathrooms']\n",
    "tt_df['bed_bath_ration'] = tt_df['bedrooms'] / tt_df['bathrooms']\n",
    "tt_df['bed_bath_ration'] = tt_df['bed_bath_ration'].replace([np.inf], np.max(tt_df.loc[tt_df['bed_bath_ration'] != np.inf, 'bed_bath_ration']) + 1)\n",
    "tt_df['bed_bath_ration'].fillna(0, inplace = True)\n",
    "\n",
    "tt_df['bath_is_int'] = (0 == tt_df['bathrooms'] % 1).astype(np.int64)\n",
    "# tt_df['diff_rooms_photos'] = tt_df['num_photos'] - tt_df['bad_plus_bath']\n",
    "tt_df['bed_bath_photos_ration'] = tt_df['bad_plus_bath'] / tt_df['num_photos']\n",
    "tt_df['bed_bath_photos_ration'] = tt_df['bed_bath_photos_ration'].replace([np.inf], np.max(tt_df.loc[tt_df['bed_bath_photos_ration'] != np.inf, 'bed_bath_photos_ration']) + 1)\n",
    "tt_df['bed_bath_photos_ration'] = tt_df['bed_bath_photos_ration'].replace([np.nan], 0)\n",
    "#---------------------------------------------------------------------------\n",
    "# Price\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['price_per_bed'] = tt_df['price'] / tt_df['bedrooms']\n",
    "tt_df['price_per_bed'] = tt_df['price_per_bed'].replace([np.inf], np.max(tt_df.loc[tt_df['price_per_bed'] != np.inf, 'price_per_bed']) + 1)\n",
    "tt_df['price_per_bath'] = tt_df['price'] / tt_df['bathrooms']\n",
    "tt_df['price_per_bath'] = tt_df['price_per_bath'].replace([np.inf], np.max(tt_df.loc[tt_df['price_per_bath'] != np.inf, 'price_per_bath']) + 1)\n",
    "tt_df['price_per_bed_plus_bath'] = tt_df['price'] / (tt_df['bedrooms'] + tt_df['bathrooms'])\n",
    "tt_df['price_per_bed_plus_bath'] = tt_df['price_per_bed_plus_bath'].replace([np.inf], np.max(tt_df.loc[tt_df['price_per_bed_plus_bath'] != np.inf, 'price_per_bed_plus_bath']) + 1)\n",
    "tt_df['price_per_photo'] = tt_df['price'] / tt_df['num_photos']\n",
    "tt_df['price_per_photo'] = tt_df['price_per_photo'].replace([np.inf], np.max(tt_df.loc[tt_df['price_per_photo'] != np.inf, 'price_per_photo']) + 1)\n",
    "#---------------------------------------------------------------------------\n",
    "# Address (case may contain info)\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['street_address'] = tt_df['street_address'].apply(lambda x: x.lower())\n",
    "tt_df['display_address'] = tt_df['display_address'].apply(lambda x: x.lower())\n",
    "# tt_df['disp_addr_is_not_in_street_addr'] = tt_df[['street_address', 'display_address']].apply(lambda x: np.int(-1 == x.street_address.find(x.display_address)), axis = 1)\n",
    "#---------------------------------------------------------------------------\n",
    "# Lat/Lon\n",
    "#---------------------------------------------------------------------------\n",
    "# # latlon count (density of points)\n",
    "tt_df['latlon'] = tt_df['longitude'].round(3).astype(str) + '_' + tt_df['latitude'].round(3).astype(str)\n",
    "\n",
    "latlon_count = tt_df['latlon'].value_counts()\n",
    "latlon_count = latlon_count.reset_index().rename(columns = {'index':'latlon', 'latlon':'density'})\n",
    "tt_df = pd.merge(tt_df, latlon_count, on = 'latlon', how = 'left')\n",
    "\n",
    "# Distance to New-Yourk center\n",
    "center_lat = 40.785091\n",
    "center_lon = -73.968285\n",
    "tt_df['euclid_dist_to_center'] = np.sqrt((tt_df['latitude'] - center_lon) ** 2  + (tt_df['longitude'] - center_lat) ** 2)\n",
    "    \n",
    "# Rotation for different angles\n",
    "for angle in [15,30,45,60]:\n",
    "    namex = 'rot' + str(angle) + '_x'\n",
    "    namey = 'rot' + str(angle) + '_y'\n",
    "    alpha = np.pi / (180 / angle)\n",
    "    \n",
    "    tt_df[namex] = tt_df['latitude'] * np.cos(alpha) + tt_df['longitude'] * np.sin(alpha)\n",
    "    tt_df[namey] = tt_df['longitude'] * np.cos(alpha) - tt_df['latitude'] * np.sin(alpha)\n",
    "    \n",
    "#---------------------------------------------------------------------------\n",
    "# Categotical\n",
    "#---------------------------------------------------------------------------\n",
    "# Label encoding\n",
    "categorical_cols = ['display_address', 'manager_id', 'building_id', 'street_address']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    tt_df.loc[:, col] = le.fit_transform(tt_df[col].values)\n",
    "    \n",
    "# Manager count\n",
    "man_count = tt_df['manager_id'].value_counts()\n",
    "man_count = man_count.reset_index().rename(columns = {'index':'manager_id', 'manager_id':'man_count'})\n",
    "tt_df = pd.merge(tt_df, man_count, on = 'manager_id', how = 'left')\n",
    "\n",
    "# Building count\n",
    "build_count = tt_df['building_id'].value_counts()\n",
    "build_count = build_count.reset_index().rename(columns = {'index':'building_id', 'building_id':'build_count'})\n",
    "tt_df = pd.merge(tt_df, build_count, on = 'building_id', how = 'left')\n",
    "\n",
    "# Top5 building\n",
    "build_count = tt_df['building_id'].value_counts()\n",
    "p = np.percentile(build_count.values, 95)\n",
    "tt_df['top_5_building'] = tt_df['building_id'].apply( lambda x: np.int(x in build_count.index.values[build_count.values >= p]) )\n",
    "#---------------------------------------------------------------------------\n",
    "# Dscription\n",
    "# Description in fact is the list of features, so probably it can add little values to 'features'\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['number_of_new_lines'] = tt_df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "tt_df['website_redacted'] = tt_df['description'].str.contains('website_redacted').astype(np.int)\n",
    "#---------------------------------------------------------------------------\n",
    "# Strange\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['price_is_round_sousand'] = (0 == tt_df['price'] % 1000).astype(np.int64)\n",
    "tt_df['price_is_round_hundred'] = (0 == tt_df['price'] % 100).astype(np.int64)\n",
    "#---------------------------------------------------------------------------\n",
    "# Image timestamp ('magic feature')\n",
    "#---------------------------------------------------------------------------\n",
    "tt_df['ts_date'] = pd.to_datetime(tt_df['timestamp'], unit = 's')\n",
    "\n",
    "# tt_df['ts_days_passed'] = (tt_df['ts_date'].max() - tt_df['ts_date']).astype('timedelta64[D]').astype(int)\n",
    "tt_df['ts_month'] = tt_df['ts_date'].dt.month\n",
    "tt_df['ts_week'] = tt_df['ts_date'].dt.week\n",
    "tt_df['ts_day'] = tt_df['ts_date'].dt.day\n",
    "# tt_df['ts_dayofweek'] = tt_df['ts_date'].dt.dayofweek\n",
    "tt_df['ts_dayofyear'] = tt_df['ts_date'].dt.dayofyear\n",
    "tt_df['ts_hour'] = tt_df['ts_date'].dt.hour\n",
    "tt_df['ts_tensdays'] = tt_df['ts_day'].apply(lambda x: 1 if x < 10 else 2 if x < 20 else 3)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Check NaNs\n",
    "#---------------------------------------------------------------------------\n",
    "print(tt_df.shape) # (124011, 60)\n",
    "print('NaN: %s' % tt_df.isnull().mean().any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset to calculate features based on taraget variable (probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = tt_df[:r]\n",
    "test_df = tt_df[r:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features based on taraget variable (probabilities)\n",
    "** * Perform groupping of interest_level by (manager_id and interest_level) * **  \n",
    "** * Should be very careful to avoid leakage. For example more than 5 folds will increase possibility of leakage * **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prob(df, col = None, agg_func = None):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    df - Panadas dataframe\n",
    "    col - column of interest\n",
    "    agg_func - aggregation function\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Pandas dataframe ready to merge with df on manager_id\n",
    "    \n",
    "    Logic\n",
    "    -----\n",
    "    \n",
    "    We have this:                      We want to get this:\n",
    "    -------------                      --------------------\n",
    "    \n",
    "    interest_level manager_id          manager_id  prob_high  prob_low  prob_medium\n",
    "               low        foo                 bar   0.333333       NaN     0.666667\n",
    "            medium        bar                 foo   0.200000       0.4     0.400000\n",
    "            medium        foo\n",
    "              high        bar\n",
    "            medium        foo\n",
    "            medium        bar\n",
    "               low        foo\n",
    "              high        foo\n",
    "    \"\"\"\n",
    "    aggregate_df = df.groupby(['manager_id', 'interest_level'])[[col]].aggregate(agg_func).rename(columns = {col: 'aggregate'}).reset_index()\n",
    "    sum_df = aggregate_df.groupby(['manager_id'])[['aggregate']].sum().rename(columns = {'aggregate': 'sum'}).reset_index()\n",
    "    aggregate_df = pd.merge(aggregate_df, sum_df, on = 'manager_id', how = 'left')\n",
    "    aggregate_df['prob'] = aggregate_df['aggregate'] / aggregate_df['sum']\n",
    "    piv_df = pd.pivot_table(aggregate_df, values='prob', columns=['interest_level'], index = 'manager_id').reset_index()\n",
    "    name = col + '_' + agg_func\n",
    "    piv_df.rename(columns = {'high': 'prob_high_' + name, 'low': 'prob_low_' + name, 'medium': 'prob_medium_' + name}, inplace = True)\n",
    "    return piv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold done\n",
      "Fold done\n",
      "Fold done\n",
      "Fold done\n",
      "Fold done\n",
      "(49352, 63)\n",
      "(74659, 63)\n"
     ]
    }
   ],
   "source": [
    "# Init CV\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "# Init aggregation\n",
    "col = 'interest_level'\n",
    "agg_func = 'count'\n",
    "\n",
    "# Init new columns\n",
    "for i in ['prob_high_', 'prob_low_', 'prob_medium_']: # alphabetically\n",
    "    train_df.loc[:, i + col + '_' + agg_func] = fill_val\n",
    "\n",
    "# For train set\n",
    "for train_index, test_index in kf.split(train_df):\n",
    "    tr_df = train_df.iloc[train_index]\n",
    "    te_df = train_df.iloc[test_index]\n",
    "\n",
    "    piv_df = get_prob(tr_df, col = col, agg_func = agg_func)\n",
    "    te_df = pd.merge(te_df, piv_df, on = 'manager_id', how = 'left')\n",
    "    train_df.iloc[test_index, -3:] = te_df.iloc[:, -3:].values\n",
    "    print('Fold done')\n",
    "\n",
    "# For test set\n",
    "piv_df = get_prob(train_df, col = col, agg_func = agg_func)\n",
    "test_df = pd.merge(test_df, piv_df, on = 'manager_id', how = 'left')\n",
    "\n",
    "# Fill NaN\n",
    "train_df.fillna(fill_val, inplace = True)\n",
    "test_df.fillna(fill_val, inplace = True)\n",
    "\n",
    "print(train_df.shape) # (49352, 63)\n",
    "print(test_df.shape) # (74659, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[:, 'interest_level'] = train_df['interest_level'].map({'high': 0, 'medium': 1, 'low': 2})\n",
    "y_train = train_df['interest_level'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine train and test to work with text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 63)\n"
     ]
    }
   ],
   "source": [
    "# Combine train and test\n",
    "tt_df = pd.concat([train_df, test_df], ignore_index = True)\n",
    "print(tt_df.shape) # (124011, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text features (sparse) from 'features' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text features from 'features'\n",
    "tt_df['features'] = tt_df['features'].apply(lambda x: ' '.join(['_'.join(i.split(' ')) for i in x]))\n",
    "vectorizer = CountVectorizer(stop_words = 'english', max_features = 200)\n",
    "tt_sparse = vectorizer.fit_transform(tt_df['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text features from 'description' column (sentiment, NO improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text features from 'description'\n",
    "# tt_df['sentiment_polarity'] = tt_df['description'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "# tt_df['sentiment_subjectivity'] = tt_df['description'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final dataset\n",
    "\n",
    "* Explicitly select columns to use\n",
    "* Combine dense and sparse data into a single sparse dataset\n",
    "* Check consistency: NaN, +INF, -INF, constant columns, duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN   ->  False\n",
      "+INF  ->  False\n",
      "-INF  ->  False\n",
      "CONST ->  False\n",
      "DUPL  ->  False\n",
      "SHAPE ->  (49352, 255) (74659, 255)\n"
     ]
    }
   ],
   "source": [
    "X_cols = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', \n",
    "          'listing_id', 'num_photos', 'num_features', 'num_description_words', \n",
    "          'month', 'day', 'hour', 'bad_plus_bath', 'more_bed', \n",
    "          'more_bath', 'bed_bath_equal', 'bed_bath_diff', 'bed_bath_ration', \n",
    "          'price_per_bed', 'price_per_bath', 'price_per_bed_plus_bath', \n",
    "          'price_per_photo', 'price_is_round_sousand', 'price_is_round_hundred', \n",
    "          'building_id_is_zero', 'bath_is_int', 'bed_bath_photos_ration', \n",
    "          'density', 'euclid_dist_to_center', \n",
    "          'prob_high_interest_level_count', 'prob_low_interest_level_count', 'prob_medium_interest_level_count', \n",
    "          'display_address', 'manager_id', 'building_id', 'street_address', \n",
    "          'man_count', 'build_count', 'top_5_building', \n",
    "          \n",
    "          'ts_month', 'ts_week', 'ts_day', 'ts_dayofyear', 'ts_hour', 'ts_tensdays',\n",
    "          \n",
    "          'rot15_x', 'rot15_y', 'rot30_x', 'rot30_y', \n",
    "          'rot45_x', 'rot45_y', 'rot60_x', 'rot60_y',\n",
    "          \n",
    "          'number_of_new_lines',  'website_redacted',\n",
    "          ]\n",
    "\n",
    "TT = sparse.hstack([tt_df[X_cols], tt_sparse]).tocsr()\n",
    "# TT = sparse.csr_matrix(tt_df[X_cols]) # without text features\n",
    "\n",
    "# Check for NaN, INF, -INF\n",
    "print('NaN   -> ', np.bool(np.mean(np.isnan(TT.toarray())))) # should be False\n",
    "print('+INF  -> ', np.bool(np.mean(np.isinf(TT.toarray())))) # should be False\n",
    "print('-INF  -> ', np.bool(np.mean(np.isneginf(TT.toarray())))) # should be False\n",
    "\n",
    "# Check for constant fetures\n",
    "print('CONST -> ', np.bool(np.mean(TT[0] == TT.mean(axis = 0)))) # should be False\n",
    "\n",
    "# Check for duplicate entries in column (feature) list\n",
    "print('DUPL  -> ', len(X_cols) != len(set(X_cols))) # should be False\n",
    "\n",
    "# Split\n",
    "X_train = TT[:r]\n",
    "X_test = TT[r:]\n",
    "\n",
    "# Shape\n",
    "print('SHAPE -> ', X_train.shape, X_test.shape) # (49352, 255) (74659, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize cross-validation for manual parameter tuning and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.526319848023\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "model = XGBClassifier(seed = 0, objective = 'multi:softprob', \n",
    "                      learning_rate = 0.1, n_estimators = 100, \n",
    "                      max_depth = 6, min_child_weight = 1, \n",
    "                      subsample = 0.7, colsample_bytree = 0.7)\n",
    "\n",
    "# Crate sklearn scorer\n",
    "scorer = make_scorer(log_loss, needs_proba = True)\n",
    "# Run CV and get mean score\n",
    "print(np.mean(cross_val_score(model, X_train, y_train, cv = 3, scoring = scorer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create out-of-fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oof(model, X_train, y_train, X_test, oof_test = True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Self-explanatory\n",
    "    oof_test - if True, then predict test set\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    S_train - OOF predictions for train set\n",
    "    S_test  - prediction for test set (fit model on full train set)\n",
    "    \"\"\"\n",
    "    # Init CV\n",
    "    kf = KFold(n_splits = 3, shuffle = True, random_state = 0)\n",
    "    \n",
    "    # Create empty numpy arrays for stacking features\n",
    "    S_train = np.zeros((X_train.shape[0], 3))\n",
    "    S_test = np.zeros((X_test.shape[0], 3))\n",
    "    \n",
    "    # Create oof predictions for train set\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        X_tr = X_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        X_te = X_train[test_index]\n",
    "        y_te = y_train[test_index]\n",
    "\n",
    "        model = model.fit(X_tr, y_tr)\n",
    "        y_te_pred = model.predict_proba(X_te)\n",
    "        S_train[test_index, :] = y_te_pred\n",
    "        print( 'Fold %d: %.6f' % (i, log_loss(y_te, y_te_pred)) )\n",
    "\n",
    "    # Score over full dataset (mean)\n",
    "    print( 'Mean:   %.6f' % log_loss(y_train, S_train) )\n",
    "    \n",
    "    # Create prediction for test set (fit on full train)\n",
    "    if oof_test:\n",
    "        model = model.fit(X_train, y_train)\n",
    "        S_test = model.predict_proba(X_test)\n",
    "    \n",
    "    return (S_train, S_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-level model 1: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine number of rounds for XGBoost using native CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08467+0.0001059\ttest-mlogloss:1.08498+1.79072e-05\n",
      "[100]\ttrain-mlogloss:0.595726+0.0015108\ttest-mlogloss:0.623256+0.00232762\n",
      "[200]\ttrain-mlogloss:0.511747+0.00196907\ttest-mlogloss:0.560258+0.00350973\n",
      "[300]\ttrain-mlogloss:0.471398+0.00201596\ttest-mlogloss:0.539507+0.00373502\n",
      "[400]\ttrain-mlogloss:0.44366+0.00189932\ttest-mlogloss:0.529379+0.00406813\n",
      "[500]\ttrain-mlogloss:0.421378+0.0019183\ttest-mlogloss:0.523286+0.00428096\n",
      "[600]\ttrain-mlogloss:0.401781+0.00162214\ttest-mlogloss:0.519346+0.00460421\n",
      "[700]\ttrain-mlogloss:0.384774+0.00158116\ttest-mlogloss:0.516953+0.0046341\n",
      "[800]\ttrain-mlogloss:0.368272+0.00140177\ttest-mlogloss:0.515245+0.00461703\n",
      "[900]\ttrain-mlogloss:0.353706+0.00137418\ttest-mlogloss:0.514096+0.00459792\n",
      "[1000]\ttrain-mlogloss:0.33996+0.00159109\ttest-mlogloss:0.513372+0.00468664\n",
      "[1100]\ttrain-mlogloss:0.326861+0.00135521\ttest-mlogloss:0.512802+0.00463329\n",
      "[1200]\ttrain-mlogloss:0.314306+0.00125541\ttest-mlogloss:0.512635+0.00481094\n",
      "\n",
      "cv mean + std -> [0.512597 + 0.004890]\n",
      "ntrees -> [1251]\n",
      "ntrees for full data (+1/3) -> [1668]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {'seed': 0,\n",
    "          'objective': 'multi:softprob',\n",
    "          'eval_metric': 'mlogloss',\n",
    "          'num_class': 3,\n",
    "          'eta': 0.02,\n",
    "          'max_depth': 6,\n",
    "          'min_child_weight': 1,\n",
    "          'subsample': 0.7,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'silent': 1,\n",
    "}\n",
    "\n",
    "# Convert data to DMatrices\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Using 3-fold CV\n",
    "res = xgb.cv(\n",
    "    params, \n",
    "    dtrain,\n",
    "    num_boost_round = 10000,\n",
    "    early_stopping_rounds = 50,\n",
    "    nfold = 3,\n",
    "    seed = 0,\n",
    "    stratified = False,\n",
    "    show_stdv = True,\n",
    "    verbose_eval = 100\n",
    "    )\n",
    "\n",
    "# Output result\n",
    "n_part = res.shape[0]\n",
    "n_full = np.int(res.shape[0] + (1/3) * res.shape[0])\n",
    "print('\\ncv mean + std -> [%.6f + %.6f]\\nntrees -> [%d]\\nntrees for full data (+1/3) -> [%d]' % (res.iloc[-1, 0], res.iloc[-1, 1], n_part, n_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 0.510092\n",
      "Fold 1: 0.509996\n",
      "Fold 2: 0.519510\n",
      "Mean:   0.513199\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "model = XGBClassifier(seed = 0, objective = 'multi:softprob', \n",
    "                      learning_rate = 0.02, n_estimators = n_part, \n",
    "                      max_depth = 6, min_child_weight = 1, \n",
    "                      subsample = 0.7, colsample_bytree = 0.7)\n",
    "# Get oof\n",
    "xgb_oof_train, xgb_oof_test = oof(model, X_train, y_train, X_test, oof_test = False)\n",
    "\n",
    "# Init model for test (as we train on full train set we need more rounds)\n",
    "model = XGBClassifier(seed = 0, objective = 'multi:softprob', \n",
    "                      learning_rate = 0.02, n_estimators = n_full, \n",
    "                      max_depth = 6, min_child_weight = 1, \n",
    "                      subsample = 0.7, colsample_bytree = 0.7)\n",
    "\n",
    "# Fit model on full train\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Predict test\n",
    "xgb_oof_test = model.predict_proba(X_test)\n",
    "\n",
    "# Export to txt files\n",
    "np.savetxt(data_dir + 'xgb_oof_train.csv', xgb_oof_train, delimiter = ',', fmt = '%.5f')\n",
    "np.savetxt(data_dir + 'xgb_oof_test.csv', xgb_oof_test, delimiter = ',', fmt = '%.5f') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-level model 2: StackNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for StackNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# First column in train - labels\n",
    "# First column in test - dummy (indices)\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Get test index to use as first dummy column in test set for StackNet\n",
    "ids = test_df['listing_id'].values\n",
    "\n",
    "# Concat oof and predictions from best model (xgb)\n",
    "TT_dense = np.c_[TT.toarray(), np.r_[xgb_oof_train, xgb_oof_test]] # (124011, 258)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "TT_dense = scaler.fit_transform(TT_dense)\n",
    "\n",
    "# Split\n",
    "X_train_dense = TT_dense[:r] # (49352, 258)\n",
    "X_test_dense = TT_dense[r:] # (74659, 258)\n",
    "\n",
    "# Append target to train\n",
    "X_train_dense = np.c_[y_train, X_train_dense] # (49352, 259)\n",
    "# Append id to test\n",
    "X_test_dense = np.c_[ids, X_test_dense] # (74659, 259)\n",
    "\n",
    "# Export to txt files\n",
    "np.savetxt(data_dir + 'train_std.csv', X_train_dense, delimiter = ',', fmt = '%.5f')\n",
    "np.savetxt(data_dir + 'test_std.csv', X_test_dense, delimiter = ',', fmt = '%.5f') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run StackNet\n",
    "** * We use dummy 3-level model in file `params.txt` just to get train oof from 2-level model * **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run StackNet and get output\n",
    "stacknet_log = check_output(['bash', 'run.sh']).decode(sys.stdout.encoding)\n",
    "# Save output to file\n",
    "with open(data_dir + 'stacknet_log.txt', 'w') as f:\n",
    "    str_len = f.write(stacknet_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load StackNet OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load StackNet oof\n",
    "stacknet_oof_train = np.loadtxt('stacknet_oof2.csv', delimiter = ',')\n",
    "stacknet_oof_test = np.loadtxt('stacknet_oof_test2.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-level model 3: Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 0.520589\n",
      "Fold 1: 0.524397\n",
      "Fold 2: 0.529433\n",
      "Mean:   0.524806\n"
     ]
    }
   ],
   "source": [
    "# Inint model\n",
    "model = ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 1000, \n",
    "                               criterion = 'entropy', max_depth = None)\n",
    "\n",
    "# Get oof\n",
    "et_oof_train, et_oof_test = oof(model, np.c_[X_train_dense[:, 1:], stacknet_oof_train], \n",
    "                                y_train, np.c_[X_test_dense[:, 1:], stacknet_oof_test], oof_test = True)\n",
    "\n",
    "# Export to txt files\n",
    "np.savetxt(data_dir + 'et_oof_train.csv', et_oof_train, delimiter = ',', fmt = '%.5f')\n",
    "np.savetxt(data_dir + 'et_oof_test.csv', et_oof_test, delimiter = ',', fmt = '%.5f') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at oof scores for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB:      0.513199\n",
      "StackNet: 0.510491\n",
      "ET:       0.524806\n"
     ]
    }
   ],
   "source": [
    "# Output oof scores\n",
    "print('XGB:      %.6f' % log_loss(y_train, xgb_oof_train))\n",
    "print('StackNet: %.6f' % log_loss(y_train, stacknet_oof_train))\n",
    "print('ET:       %.6f' % log_loss(y_train, et_oof_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform SLSQP optimization with bounds and constraints (9 parameters for each column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# One parameter for each column\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def cost(params):\n",
    "    y_pred = params[:3] * xgb_oof_train + params[3:6] * stacknet_oof_train + params[6:9] * et_oof_train\n",
    "    return log_loss(y_train, y_pred)\n",
    "    \n",
    "def con1(params):\n",
    "    return params[0] + params[3] + params[6] - 1\n",
    "    \n",
    "def con2(params):\n",
    "    return params[1] + params[4] + params[7] - 1\n",
    "    \n",
    "def con3(params):\n",
    "    return params[2] + params[5] + params[8] - 1\n",
    "\n",
    "# params = [0.33] * 9\n",
    "# print(cost(params)) # 0.511137\n",
    "\n",
    "n = 9\n",
    "init = [0.33] * n\n",
    "cons = ({'type': 'eq', 'fun': con1},\n",
    "        {'type': 'eq', 'fun': con2},\n",
    "        {'type': 'eq', 'fun': con3})\n",
    "bounds = [(0, 1)] * n\n",
    "res = minimize(cost, init, method = 'SLSQP', bounds = bounds, constraints = cons, options = {'maxiter': 100000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.5087084708118806\n",
      "     jac: array([-0.00132257, -0.00050984,  0.00180638, -0.00122941, -0.00069389,\n",
      "        0.0020479 , -0.00132313, -0.00065529,  0.00200661])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 144\n",
      "     nit: 13\n",
      "    njev: 13\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.29034443,  0.2517158 ,  0.54287206,  0.50961993,  0.62058988,\n",
      "        0.45712794,  0.20003563,  0.12769432,  0.        ])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = res['x']\n",
    "y_pred = params[:3] * xgb_oof_test + params[3:6] * stacknet_oof_test + params[6:9] * et_oof_test\n",
    "        \n",
    "subm_df.loc[:, 'listing_id'] = test_df['listing_id'].values\n",
    "subm_df.iloc[:, 1:] = y_pred\n",
    "subm_df.to_csv('./reproduced_submission/reproduced_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ** * Relatively simple solution with strong result * **  \n",
    "2. ** * A lot of fun with cool dataset and competition* **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
